Hadoop and Spark manage distributed data processing by utilizing a Master-Slave architecture to handle massive volumes of
data across multiple computers (nodes) in a cluster. While Hadoop focuses on a combination of distributed storage and 
batch processing, Spark provides a faster, unified engine for both batch and real-time streaming data

**Hadoop’s Core Management Components**

Hadoop manages distributed processing through three primary layers: storage, resource management, and the processing engine itself.

• HDFS (Hadoop Distributed File System): This is the storage layer. It breaks large files into blocks (default size is often 128MB) and distributes them across various DataNodes. A master node, called the NameNode, manages the metadata (the information about where each block is stored). To ensure fault tolerance, HDFS uses replication, creating multiple copies of each data block so that if one node fails, the data remains accessible from another

• YARN (Yet Another Resource Negotiator): This component acts as the "operating system" of the cluster, managing resources and scheduling jobs. The ResourceManager (Master) allocates resources (CPU and memory), while the NodeManager (Slave) monitors the resource usage of individual nodes.

• MapReduce: This is the traditional processing framework. It divides a task into two phases: the Map phase, where data is filtered and sorted across nodes, and the Reduce phase, where the results are aggregated. It is designed primarily for batch processing rather than real-time data
